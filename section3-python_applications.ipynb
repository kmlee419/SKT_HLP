{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"colab":{"name":"section3-python_applications.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"kx7yp860pLh-"},"source":["# Part 1 : Natural Language Processing"]},{"cell_type":"code","metadata":{"id":"4sqhT2HCpLiE"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import sklearn\n","from sklearn.model_selection import train_test_split\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L9PTa1tCpLiG"},"source":["## 1.1 Data\n","\n","* In this lecture, we use the SMS Spam Collection Data Set from UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection). \n","    * A collection of 425 SMS spam messages was manually extracted from the Grumbletext Web site.\n","    * A subset of 3,375 SMS randomly chosen ham messages of the NUS SMS Corpus (NSC), which is a dataset of about 10,000 legitimate messages collected for research at the Department of Computer Science at the National University of Singapore."]},{"cell_type":"code","metadata":{"id":"1-YgEplBpLiH"},"source":["#df_sms = pd.read_csv('./SMS_Spam.tsv', sep='\\t')\n","\n","import io\n","from google.colab import files\n","uploaded = files.upload()\n","\n","df_sms = pd.read_csv(io.StringIO(uploaded['SMS_Spam.tsv'].decode('utf-8')), sep='\\t')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RyBMO4CmpLiH"},"source":["df_sms.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZNypc4CMpLiI"},"source":["## 1.2 Exploratory Data Analysis\n","* First, how many messages the data have?"]},{"cell_type":"code","metadata":{"id":"lRlxbKxdpLiI"},"source":["len(df_sms)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jEu3tgrOpLiJ"},"source":["* Then, now, how many spams and hams each other?"]},{"cell_type":"code","metadata":{"id":"7-KZXF-ipLiJ"},"source":["df_sms['label'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gLwfUERSpLiK"},"source":["* Now, let's apply lengths of each message and create a new column."]},{"cell_type":"code","metadata":{"id":"YkJ0MR3WpLiK"},"source":["df_sms['length'] = df_sms['message'].apply(len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s7xBEUllpLiL"},"source":["df_sms.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5ufnb5kjpLiL"},"source":["* How are the lengths of messages distributed?"]},{"cell_type":"code","metadata":{"id":"6lljkTBVpLiL"},"source":["sns.displot(df_sms['length'])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CYElnLQhpLiL"},"source":["* Are there any differences of the distribution of spam and ham messages?"]},{"cell_type":"code","metadata":{"id":"HsjV3O1QpLiM"},"source":["df_spam = df_sms[df_sms['label']=='spam'].reset_index(drop=True)\n","df_ham = df_sms[df_sms['label']=='ham'].reset_index(drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T4CdTSw0pLiM"},"source":["plt.figure(figsize=(15,10))\n","\n","sns.displot(df_spam['length'], color='red')\n","sns.displot(df_ham['length'], color='blue')\n","plt.legend(labels=['spam','ham'])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-T2k2S7wpLiM"},"source":["## 1.3 Text preprocessing\n","* For analyzing texts, we need to split each message into individual words.\n","* Let's remove punctuations first.\n","    * Python's built-in library **string** would provide a quick and convenient way of removing them."]},{"cell_type":"code","metadata":{"id":"F7NWFFnYpLiN"},"source":["import string\n","\n","string.punctuation"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iOcz6L0apLiN"},"source":["* Check characters whether they are punctuations or not."]},{"cell_type":"code","metadata":{"id":"bAw4FtlNpLiN"},"source":["sample = \"Hello! This is SK HLP: Data Literacy lecture.\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"59c_YQEppLiO"},"source":["sample_nopunc = []\n","for char in sample:\n","    if char not in string.punctuation:\n","        sample_nopunc.append(char)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"395NE3cspLiO"},"source":["sample_nopunc = \"\".join(sample_nopunc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GYI03YcspLiO"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ox264rEBpLiO"},"source":["* Now, it's a step to remove stopwords. The NLTK library is a kind of stardard library for processing texts in Python (https://www.nltk.org/).\n","* The NLTK library provide a list of stopwords."]},{"cell_type":"code","metadata":{"id":"h18JaLsmpLiP"},"source":["import nltk\n","from nltk.corpus import stopwords"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FHryyOWxpLiP"},"source":["* We can specify a language for stopwords list."]},{"cell_type":"code","metadata":{"id":"hhM42SFnpLiP"},"source":["nltk.download('stopwords')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xKoy4gynpLiQ"},"source":["stopwords.words('english')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Zbr0-OPpLiQ"},"source":["* Split the message and remove stopwords according to the list."]},{"cell_type":"code","metadata":{"id":"UJljrX7_pLiQ"},"source":["sample_nopunc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pRw34IlHpLiR"},"source":["sample_nopunc.split()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B_0OSCI8pLiR"},"source":["remove_stopwords = []\n","for word in sample_nopunc.split():\n","    if word.lower() not in stopwords.words('english'):\n","        remove_stopwords.append(word)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e53YMRuHpLiS"},"source":["remove_stopwords"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_t3E456vpLiS"},"source":["* When you make a function for this, it would be more useful to apply it later."]},{"cell_type":"code","metadata":{"id":"gS4H9ZHzpLiS"},"source":["def preprocessing(text):\n","    \n","    # remove punctuation\n","    nopunc = []\n","    for char in text:\n","        if char not in string.punctuation:\n","            nopunc.append(char)\n","            \n","    nopunc = \"\".join(nopunc)\n","    \n","    # remove stopwords\n","    remove_stop = []\n","    for word in nopunc.split():\n","        if word.lower() not in stopwords.words('english'):\n","            remove_stop.append(word)\n","            \n","    # remove words less than three characters\n","    tokens = []\n","    for word in remove_stop:\n","        if len(word) >= 3:\n","            tokens.append(word)\n","            \n","    #tokens = \" \".join(tokens)\n","    \n","    return tokens"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MKZrKiispLiT"},"source":["sample"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"iT043QLEpLiT"},"source":["preprocessing(sample)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"98mBdQgFpLiU"},"source":["* You can apply the preprocessing function to whole dataframe."]},{"cell_type":"code","metadata":{"id":"M1pPlgmRpLiU"},"source":["df_sms.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GW6jwiWFpLiU"},"source":["df_sms['message'].apply(preprocessing)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ic_knuwOpLiV"},"source":["## 1.4 Frequency Analysis"]},{"cell_type":"code","metadata":{"id":"_lw0NxD2pLiV"},"source":["clean_spam = df_spam['message'].apply(preprocessing)\n","clean_ham = df_ham['message'].apply(preprocessing)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"47JSixSKpLiV"},"source":["* First, let's merge whole values of each dataframe into one list."]},{"cell_type":"code","metadata":{"id":"IhHZuJQ4pLiV"},"source":["whole_spam = []\n","for line in clean_spam.tolist():\n","    whole_spam += line\n","    \n","whole_ham = []\n","for line in clean_ham.tolist():\n","    whole_ham += line"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aAZ-_kMspLiW"},"source":["* The **Text** class in **NLTK** library provide some useful methods to text analysis."]},{"cell_type":"code","metadata":{"id":"B_qZDLhfpLiW"},"source":["from nltk import Text\n","\n","ham_text = Text(whole_ham)\n","spam_text = Text(whole_spam)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aouAFbmvpLiW"},"source":["* The **vocab** method in the **Text** class can extract the frequency of usage for each token."]},{"cell_type":"code","metadata":{"id":"RRVjY9mxpLiW"},"source":["freqDist_ham = ham_text.vocab()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"55V6cBoRpLiX"},"source":["freqDist_ham.most_common(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3zltNPoSpLiX"},"source":["* How about spam messages?"]},{"cell_type":"code","metadata":{"id":"5TMs9Fv5pLiX"},"source":["freqDist_spam = spam_text.vocab()\n","freqDist_spam.most_common(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eKQBB7Y_pLiX"},"source":["* You can plot the distribution of each token by the **plot** method."]},{"cell_type":"code","metadata":{"id":"kmmLhsOnpLiY"},"source":["plt.figure(figsize=(10,8))\n","\n","ham_text.plot(30)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XBIjJFmGpLiY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hrp7LgcMpLiY"},"source":["* We can also use the **wordcloud** package for visualization. \n","* You can download the package by `conda install -c conda-forge wordcloud`"]},{"cell_type":"code","metadata":{"id":"qBtbavc8pLiY"},"source":["from wordcloud import WordCloud\n","\n","plt.figure(figsize=(15,10))\n","\n","wc_ham = WordCloud(width=1000, height=600, background_color=\"black\", random_state=0)\n","plt.imshow(wc_ham.generate_from_frequencies(freqDist_ham))\n","plt.axis(\"off\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0MHhyYgcpLiZ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KAhM1Xn-pLiZ"},"source":["# Part 2 : Recommendation System"]},{"cell_type":"markdown","metadata":{"id":"DJY2WB9npLiZ"},"source":["* Recommendation system is a sort of information filtering system that seeks to predict the \"rating\" or \"preference\" a user would give to an item. They are primarily used in commercial applications (https://en.wikipedia.org/wiki/Recommender_system)\n","* There are two common types of recommender systems:\n","    * **Content-Based Filtering** focus on the attributes of the items and give you recommendations based on the similarity between them.\n","    \n","    * **Collaborative Filtering** produces recommendations based on the user's attitude (activity) to items.\n","\n","\n","* Movie recommendation is one of the first step to start learning recommendation systems.\n","* MovieLens dataset is a famous one for learning to build the recommendation systems.\n","    * https://grouplens.org/datasets/movielens/\n","    * https://kaggle.com/grouplens/movielens-20m-dataset"]},{"cell_type":"code","metadata":{"id":"9X8jpblNpLiZ"},"source":["#ratings = pd.read_csv('./ratings.csv')\n","\n","uploaded = files.upload()\n","ratings = pd.read_csv(io.StringIO(uploaded['ratings.csv'].decode('utf-8')))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xld4BDWMpLia"},"source":["ratings.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SuSjEuPmpLia"},"source":["#movies = pd.read_csv('./movies.csv')\n","\n","uploaded = files.upload()\n","movies = pd.read_csv(io.StringIO(uploaded['movies.csv'].decode('utf-8')))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QTArfebrpLia"},"source":["movies.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R3KQmFIxpLia"},"source":["* Let's first merge those two dataframes."]},{"cell_type":"code","metadata":{"id":"GDKBqcOLpLib"},"source":["df_movies = pd.merge(ratings, movies, on='movieId')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bZzmGCUEpLib"},"source":["df_movies.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TqWsYSywpLib"},"source":["* Which movie has the highest user ratings on average?"]},{"cell_type":"code","metadata":{"id":"JMbIqaN-pLib"},"source":["ratings_sort = df_movies.groupby('title')['rating'].mean().sort_values(ascending=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NCRQ0sGGpLib"},"source":["ratings_sort"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z3el3_jgpLic"},"source":["* Which movies received the most ratings from users?"]},{"cell_type":"code","metadata":{"id":"pED8kUGXpLic"},"source":["counting_sort = df_movies.groupby('title')['rating'].count().sort_values(ascending=False)\n","counting_sort"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RBqkE2X7pLic"},"source":["* Let's combine of those two results."]},{"cell_type":"code","metadata":{"id":"4xbn-96FpLic"},"source":["movie_ratings = pd.DataFrame(df_movies.groupby('title')['rating'].mean())\n","movie_ratings['numbers'] = pd.DataFrame(df_movies.groupby('title')['rating'].count())\n","movie_ratings.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J-anRKHYpLic"},"source":["* Now, reshape the dataframe with using pivot_table."]},{"cell_type":"code","metadata":{"id":"sd34g5LxpLid"},"source":["user_movie_matrix = df_movies.pivot_table(index='userId', columns='title', values='rating')\n","user_movie_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_wfIljlxpLid"},"source":["* Fill the NaN values to 0."]},{"cell_type":"code","metadata":{"id":"sGz1R33-pLid"},"source":["user_movie_matrix.fillna(0, inplace=True)\n","user_movie_matrix.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vALco3JwpLid"},"source":["* Let's take two examples of movies."]},{"cell_type":"code","metadata":{"id":"xmE8gV1vpLie"},"source":["Matrix = user_movie_matrix['Matrix, The (1999)']\n","Matrix.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HERTG2KCpLie"},"source":["Terminator = user_movie_matrix['Terminator 2: Judgment Day (1991)']\n","Terminator.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gqNNaOuUpLie"},"source":["* How similar with those two movies?"]},{"cell_type":"code","metadata":{"id":"IzGSKVMCpLif"},"source":["Matrix.corr(Terminator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q4aW4eQNpLif"},"source":["* Which movie is the most similar with the \"Matrix, The (1999)\"?"]},{"cell_type":"code","metadata":{"id":"uNYgsm3fpLig"},"source":["Matrix_corr = pd.DataFrame(user_movie_matrix.corrwith(Matrix), columns=['correl'])\n","Matrix_corr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ak8eIRJQpLig"},"source":[""],"execution_count":null,"outputs":[]}]}